---
title: 'I2I: Initializing Adapters with Improvised Knowledge'
abstract: Adapters present a promising solution to the catastrophic forgetting problem
  in continual learning. However, training independent Adapter modules for every new
  task misses an opportunity for cross-task knowledge transfer. We propose Improvise
  to Initialize (I2I), a continual learning algorithm that initializes Adapters for
  incoming tasks by distilling knowledge from previously-learned tasksâ€™ Adapters.
  We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting
  experiments on sequences of visual question answering tasks.  Adapters trained with
  I2I consistently achieve better task accuracy than independently-trained Adapters,
  demonstrating that our algorithm facilitates knowledge transfer between task Adapters.
  I2I also results in better cross-task knowledge transfer than the state-of-the-art
  AdapterFusion without incurring the associated parametric cost.
year: '2023'
video: https://youtu.be/ya25hUjF8iI
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: srinivasan23a
month: 0
tex_title: 'I2I: Initializing Adapters with Improvised Knowledge'
firstpage: 923
lastpage: 935
page: 923-935
order: 923
cycles: false
bibtex_author: Srinivasan, Tejas and Jia, Furong and Rostami, Mohammad and Thomason,
  Jesse
author:
- given: Tejas
  family: Srinivasan
- given: Furong
  family: Jia
- given: Mohammad
  family: Rostami
- given: Jesse
  family: Thomason
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/srinivasan23a/srinivasan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
