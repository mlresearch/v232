---
title: Value-aware Importance Weighting for Off-policy Reinforcement Learning
abstract: Importance sampling is a central idea underlying off-policy prediction in
  reinforcement learning. It provides a strategy for re-weighting samples from a distribution
  to represent unbiased estimates of another distribution. However, importance sampling
  weights tend to be of high variance, often leading to stability issues in practice.
  In this work, we consider a broader class of importance weights to correct samples
  in off-policy learning. We propose the use of value-aware importance weights which
  take into account the sample space to provide lower variance, but still unbiased,
  estimates under a target distribution. We derive how such weights can be computed,
  and detail key properties of the resulting importance weights. We then extend several
  reinforcement learning prediction algorithms to the off-policy setting with these
  weights, and evaluate them empirically.
year: '2023'
video: https://youtu.be/-88AajCORL0
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: de-asis23a
month: 0
tex_title: Value-aware Importance Weighting for Off-policy Reinforcement Learning
firstpage: 745
lastpage: 763
page: 745-763
order: 745
cycles: false
bibtex_author: De Asis, Kristopher and Graves, Eric and Sutton, Richard S.
author:
- given: Kristopher
  family: De Asis
- given: Eric
  family: Graves
- given: Richard S.
  family: Sutton
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/de-asis23a/de-asis23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
