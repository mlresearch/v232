---
title: Continually learning representations at scale
abstract: 'Many widely used continual learning benchmarks follow a protocol that starts
  from an untrained, randomly initialized model that needs to sequentially learn a
  number of incoming tasks. To maximize interpretability of the results and to keep
  experiment length under control, often these tasks are formed from well-known medium
  to large size datasets such as CIFAR or ImageNet. Recently, however, large-scale
  pretrained representations, also referred to as foundation models, have achieved
  significant success across a wide range of traditional vision and language problems.
  Furthermore, the availability of these pretrained models and their use as starting
  point for training can be seen as a paradigm shift from the classical end-to-end
  learning.  This raises the question: How does this paradigm shift influence continual
  learning research? We attempt an answer, by firstly showing that many existing benchmarks
  are ill-equipped in this setting. The use of foundation model leads to state-of-art
  results on several existing and commonly used image classification continual learning
  benchmarks, from split CIFAR-100 to split ImageNet. Additionally, there is at best
  a small gap between keeping the representations frozen versus tuning them. While
  this is indicative of the overlap between pretraining distribution and the benchmark
  distribution, it also shows that these benchmarks can not be used to explore how
  to continually learn the underlying representations. Secondly, we examine what differentiates
  continually learning from scratch versus when relying on pretrained models, where
  the representation is learned under a different objective. We highlight that this
  brings about new challenges and research questions that cannot be studied in the
  sanitised scenario of learning from scratch explored so far. '
year: '2023'
video: https://youtu.be/pFHwqojGY9o
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: galashov23a
month: 0
tex_title: Continually learning representations at scale
firstpage: 534
lastpage: 547
page: 534-547
order: 534
cycles: false
bibtex_author: Galashov, Alexandre and Mitrovic, Jovana and Tirumala, Dhruva and Teh,
  Yee Whye and Nguyen, Timothy and Chaudhry, Arslan and Pascanu, Razvan
author:
- given: Alexandre
  family: Galashov
- given: Jovana
  family: Mitrovic
- given: Dhruva
  family: Tirumala
- given: Yee Whye
  family: Teh
- given: Timothy
  family: Nguyen
- given: Arslan
  family: Chaudhry
- given: Razvan
  family: Pascanu
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/galashov23a/galashov23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
