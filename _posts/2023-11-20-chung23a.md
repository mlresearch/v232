---
title: Incremental Unsupervised Domain Adaptation on Evolving Graphs
abstract: 'Non-stationary data distributions in evolving graphs can create problems
  for deployed graph neural networks (GNN), such as fraud detection GNNs that can
  become ineffective when fraudsters alter their patterns. The aim of this study is
  to investigate how to incrementally adapt graph neural networks to incoming, unlabeled
  graph data after training and deployment. To achieve this, we propose a new approach
  called graph contrastive self-training (GCST) that combines contrastive learning
  and self-training to alleviate performance drop. To evaluate the effectiveness of
  our approach, we conduct a comprehensive empirical evaluation on four diverse graph
  datasets, comparing it to domain-invariant feature learning methods and plain self-training
  methods. Our contribution is three-fold: we formulate and study incremental unsupervised
  domain adaptation on evolving graphs, present an approach that integrates contrastive
  learning and self-training, and conduct a comprehensive empirical evaluation of
  our approach, which demonstrates its stability and superiority over other methods.'
year: '2023'
video: https://youtu.be/y59GEQObVSA
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chung23a
month: 0
tex_title: Incremental Unsupervised Domain Adaptation on Evolving Graphs
firstpage: 683
lastpage: 702
page: 683-702
order: 683
cycles: false
bibtex_author: Chung, Hsing-Huan and Ghosh, Joydeep
author:
- given: Hsing-Huan
  family: Chung
- given: Joydeep
  family: Ghosh
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/chung23a/chung23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
