---
title: Self-trained Centroid Classifiers for Semi-supervised Cross-domain Few-shot
  Learning
abstract: State-of-the-art cross-domain few-shot learning methods for image classification
  apply knowledge transfer by fine-tuning deep feature extractors obtained from source
  domains on the small labelled dataset available for the target domain, generally
  in conjunction with a simple centroid-based classification head. Semi-supervised
  learning during the meta-test phase is an obvious approach to incorporating unlabelled
  data into cross-domain few-shot learning, but semi-supervised methods designed for
  larger sets of labelled data than those available in few-shot learning appear to
  easily go astray when applied in this setting. We propose an efficient semi-supervised
  learning method that applies self-training to the classification head only and show
  that it can yield very consistent improvements in average performance in the Meta-Dataset
  benchmark for cross-domain few-shot learning when applied with contemporary methods
  utilising centroid-based classification.
year: '2023'
video: https://youtu.be/fH3W-3D6GMc
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23a
month: 0
tex_title: Self-trained Centroid Classifiers for Semi-supervised Cross-domain Few-shot
  Learning
firstpage: 481
lastpage: 492
page: 481-492
order: 481
cycles: false
bibtex_author: Wang, Hongyu and Frank, Eibe and Pfahringer, Bernhard and Holmes, Geoffrey
author:
- given: Hongyu
  family: Wang
- given: Eibe
  family: Frank
- given: Bernhard
  family: Pfahringer
- given: Geoffrey
  family: Holmes
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/wang23a/wang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
