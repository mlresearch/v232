---
title: Reducing Communication Overhead in Federated Learning for Pre-trained Language
  Models Using Parameter-Efficient Finetuning
abstract: Pre-trained language models are shown to be effective in solving real-world
  natural language problems. Due to privacy reasons, data may not always be available
  for pre-training or finetuning of the model. Federated learning (FL) is a privacy-preserving
  technique for model training, but it suffers from communication overhead when the
  model size is large. We show that parameter-efficient finetuning (PEFT) reduces
  communication costs while achieving good model performance in both supervised and
  semi-supervised federated learning. Also, often in real life, data for the target
  downstream task is not available, but it is relatively easy to obtain the data for
  other related tasks. To this end, our results on the task-level transferability
  of PEFT methods in federated learning show that the model achieves good zero-shot
  performance on target data when source data is from a similar task. Parameter-efficient
  finetuning can aid federated learning in building efficient, privacy-preserving
  Natural Language Processing (NLP) applications.
year: '2023'
video: https://youtu.be/SwwgZ8UlIi8
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malaviya23a
month: 0
tex_title: Reducing Communication Overhead in Federated Learning for Pre-trained Language
  Models Using Parameter-Efficient Finetuning
firstpage: 456
lastpage: 469
page: 456-469
order: 456
cycles: false
bibtex_author: Malaviya, Shubham and Shukla, Manish and Lodha, Sachin
author:
- given: Shubham
  family: Malaviya
- given: Manish
  family: Shukla
- given: Sachin
  family: Lodha
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/malaviya23a/malaviya23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
