---
title: Continual Learning Beyond a Single Model
abstract: A growing body of research in continual learning focuses on the catastrophic
  forgetting problem. While many attempts have been made to alleviate this problem,
  the majority of the methods assume a \textit{single model} in the continual learning
  setup. In this work, we question this assumption and show that employing \textit{ensemble
  models} can be a simple yet effective method to improve continual performance. However,
  ensemblesâ€™ training and inference costs can increase significantly as the number
  of models grows. Motivated by this limitation, we study different ensemble models
  to understand their benefits and drawbacks in continual learning scenarios. Finally,
  to overcome the high compute cost of ensembles, we leverage recent advances in neural
  network subspace to propose a computationally cheap algorithm with similar runtime
  to a single model yet enjoying the performance benefits of ensembles.
year: '2023'
video: https://youtu.be/QWBmFkIGUls
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: doan23a
month: 0
tex_title: Continual Learning Beyond a Single Model
firstpage: 961
lastpage: 991
page: 961-991
order: 961
cycles: false
bibtex_author: Doan, Thang and Mirzadeh, Seyed Iman and Farajtabar, Mehrdad
author:
- given: Thang
  family: Doan
- given: Seyed Iman
  family: Mirzadeh
- given: Mehrdad
  family: Farajtabar
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/doan23a/doan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
