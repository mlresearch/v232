---
title: 'What Happens During Finetuning of Vision Transformers: An Invariance Based
  Investigation'
abstract: 'The pretrain-finetune paradigm usually improves downstream performance
  over training a model from scratch on the same task, becoming commonplace across
  many areas of machine learning. While pretraining is empirically observed to be
  beneficial for a range of tasks, there is not a clear understanding yet of the reasons
  for this effect. In this work, we examine the relationship between pretrained vision
  transformers and the corresponding finetuned versions on several benchmark datasets
  and tasks. We present new metrics that specifically investigate the degree to which
  invariances learned by a pretrained model are retained or forgotten during finetuning.
  Using these metrics, we present a suite of empirical findings, including that pretraining
  induces transferable invariances in shallow layers and that invariances from deeper
  pretrained layers are compressed towards shallower layers during finetuning. Together,
  these findings contribute to understanding some of the reasons for the successes
  of pretrained models and the changes that a pretrained model undergoes when finetuned
  on a downstream task.  '
year: '2023'
video: https://youtu.be/sBL6TbwdJQ0
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: merlin23a
month: 0
tex_title: 'What Happens During Finetuning of Vision Transformers: An Invariance Based
  Investigation'
firstpage: 601
lastpage: 619
page: 601-619
order: 601
cycles: false
bibtex_author: Merlin, Gabriele and Nanda, Vedant and Rawal, Ruchit and Toneva, Mariya
author:
- given: Gabriele
  family: Merlin
- given: Vedant
  family: Nanda
- given: Ruchit
  family: Rawal
- given: Mariya
  family: Toneva
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/merlin23a/merlin23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
