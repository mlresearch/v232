---
title: Challenging Common Assumptions about Catastrophic Forgetting and Knowledge
  Accumulation
abstract: Building learning agents that can progressively learn and accumulate knowledge
  is the core goal of the continual learning (CL) research field. Unfortunately, training
  a model on new data usually compromises the performance on past data. In the CL
  literature, this effect is referred to as catastrophic forgetting (CF). CF has been
  largely studied, and a plethora of methods have been proposed to address it on short
  sequences of non-overlapping tasks. In such setups, CF usually leads to a quick
  and significant drop in performance in past tasks. Nevertheless, despite CF, recent
  work showed that SGD training on linear models accumulates knowledge in a CL regression
  setup. This phenomenon becomes especially visible when tasks reoccur. We might then
  wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate
  knowledge in such a way. Such phenomena would have interesting consequences for
  applying DNNs to real continual scenarios. Indeed, standard gradient-based optimization
  methods are significantly less computationally expensive than existing CL algorithms.
  In this paper, we study the progressive knowledge accumulation (KA) in DNNs trained
  with gradient-based algorithms in long sequences of tasks with data re-occurrence.
  We propose a new framework, SCoLe (Scaling Continual Learning), to investigate KA
  and discover that catastrophic forgetting has a limited effect on DNNs trained with
  SGD. When trained on long sequences with data sparsely re-occurring, the overall
  accuracy improves, which might be counter-intuitive given our understanding of catastrophic
  forgetting in CL. We empirically investigate KA in DNNs under various data occurrence
  frequencies. We show that the catastrophic forgetting usually observed in short
  scenarios does not prevent knowledge accumulation in longer ones. Moreover, propose
  simple and scalable strategies to increase knowledge accumulation in DNNs.
year: '2023'
video: https://youtu.be/jKHYSvkqJCw
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lesort23a
month: 0
tex_title: Challenging Common Assumptions about Catastrophic Forgetting and Knowledge
  Accumulation
firstpage: 43
lastpage: 65
page: 43-65
order: 43
cycles: false
bibtex_author: Lesort, Timoth\'ee and Ostapenko, Oleksiy and Rodr\'iguez, Pau and
  Misra, Diganta and Arefin, Md Rifat and Charlin, Laurent and Rish, Irina
author:
- given: Timothée
  family: Lesort
- given: Oleksiy
  family: Ostapenko
- given: Pau
  family: Rodríguez
- given: Diganta
  family: Misra
- given: Md Rifat
  family: Arefin
- given: Laurent
  family: Charlin
- given: Irina
  family: Rish
date: 2023-11-20
address:
container-title: Proceedings of The 2nd Conference on Lifelong Learning Agents
volume: '232'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 11
  - 20
pdf: https://proceedings.mlr.press/v232/lesort23a/lesort23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
